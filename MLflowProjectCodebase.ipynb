{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of Required Libraries\n",
        "\n",
        "This cell handles the installation of the essential libraries needed for the project. It installs TensorFlow for building and training the neural network, NLTK for processing text data, and Keras-Tuner for optimizing the model's hyperparameters. This setup ensures that all necessary Python packages are available in the environment for the subsequent parts of the project.\n",
        "\n"
      ],
      "metadata": {
        "id": "mnG_4gw4BX7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow, NLTK, and Keras-Tuner for model development and tuning\n",
        "%pip install tensorflow nltk keras-tuner\n"
      ],
      "metadata": {
        "id": "lLLH5oSq_5kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart the Python Environment\n",
        "\n",
        "This cell uses Databricks' utility function `dbutils.library.restartPython()` to restart the Python environment. Restarting is crucial after installing new libraries to ensure that the newly installed packages are correctly loaded. This step helps avoid conflicts that might arise from changes in the environment since the notebook session started.\n"
      ],
      "metadata": {
        "id": "Ph9BUjnkBkds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the Python environment to ensure newly installed packages are loaded\n",
        "dbutils.library.restartPython()\n"
      ],
      "metadata": {
        "id": "bSx_rzCkBoCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Data Preparation\n",
        "\n",
        "This cell is dedicated to setting up the data pipeline. It starts by importing necessary libraries such as NLTK for natural language processing, NumPy for numerical operations, and components from TensorFlow and scikit-learn for model preparation. The NLTK library is used to download and load the 'movie_reviews' dataset, which is then preprocessed and split into training and test sets. The preprocessing steps include tokenizing the text data and converting it into sequences, which are necessary for training the neural network model.\n"
      ],
      "metadata": {
        "id": "wjQqtlQIBsRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data processing and model preparation\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download the NLTK movie reviews dataset\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Load the movie reviews and their associated categories (positive or negative)\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Unzip reviews and labels from the documents\n",
        "reviews, labels = zip(*documents)\n",
        "\n",
        "# Convert labels to a numpy array with binary encoding (1 = positive, 0 = negative)\n",
        "labels = np.array([1 if label == 'pos' else 0 for label in labels])\n",
        "\n",
        "# Initialize and configure the tokenizer to convert text to sequences of integers\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "# Transform reviews to sequences and pad them to ensure uniform length\n",
        "sequences = tokenizer.texts_to_sequences(reviews)\n",
        "data = pad_sequences(sequences, maxlen=200)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "eQ4ULYU6_8ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Model Definition\n",
        "\n",
        "In this cell, a function `build_model` is defined to create the neural network model using TensorFlow Keras. The model uses an Embedding layer for text input, followed by multiple Dense layers with ReLU activations and Dropout layers for regularization, configured dynamically using hyperparameters. The final architecture includes a Global Average Pooling layer and a sigmoid activation layer for binary classification. This setup is encapsulated in a function to facilitate hyperparameter tuning with Keras Tuner.\n"
      ],
      "metadata": {
        "id": "ZJlZ87cFCNIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow and define the model architecture using Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_model(hp):\n",
        "    \"\"\"Builds a sequential neural network model from hyperparameters.\"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(10000, 128, input_length=200))\n",
        "\n",
        "    # Add multiple dense layers based on hyperparameters with dropout for regularization\n",
        "    for i in range(hp.Int('n_layers', 1, 3)):\n",
        "        model.add(layers.Dense(units=hp.Int('n_units', min_value=64, max_value=256, step=32),\n",
        "                               activation='relu'))\n",
        "        model.add(layers.Dropout(hp.Float('dropout_rate', 0.1, 0.5)))\n",
        "\n",
        "    model.add(layers.GlobalAveragePooling1D())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model with Adam optimizer and binary cross-entropy loss\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YceNuji2ADmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Hyperparameter Tuning and MLflow Integration\n",
        "\n",
        "This cell integrates MLflow for tracking experiments and sets up hyperparameter tuning using Keras Tuner's RandomSearch. The MLflow tracking URI is set to 'databricks', ensuring that all experiment data is logged to the Databricks workspace. The hyperparameter tuning process involves defining a search space for model configurations and executing the search over a specified number of trials and epochs. The best model from the tuning process is then evaluated on the test set, and the test accuracy is logged to MLflow for performance tracking.\n"
      ],
      "metadata": {
        "id": "UoJZxgdMCgBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MLflow and Keras Tuner for model tracking and hyperparameter tuning\n",
        "import mlflow\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Set the MLflow tracking URI for Databricks\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "\n",
        "# Enable auto-logging for TensorFlow to MLflow\n",
        "mlflow.tensorflow.autolog()\n",
        "\n",
        "# Configure and initiate the RandomSearch tuner with the defined model-building function\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='SentimentAnalysis',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Start an MLflow run and perform hyperparameter tuning with training and validation\n",
        "with mlflow.start_run():\n",
        "    tuner.search(X_train, y_train, epochs=5, validation_split=0.1)\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Evaluate the best model on the test set and log the test accuracy to MLflow\n",
        "    test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_acc)\n"
      ],
      "metadata": {
        "id": "hisG2mXVAHVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Model Deployment and Lifecycle Management\n",
        "\n",
        "The final cell handles the deployment aspects of the project. It logs the best-performing model to MLflow, constructs a unique model URI, and then registers the model in the MLflow Model Registry under the name 'SentimentAnalysisModel'. After registration, the model is transitioned to the 'Production' stage using MLflow's lifecycle management capabilities. This step is critical for moving the model from a development stage to a production-ready state, ensuring it is available for real-world applications.\n"
      ],
      "metadata": {
        "id": "soxPPl5mCpzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MLflow TensorFlow utilities for model logging\n",
        "import mlflow.tensorflow\n",
        "\n",
        "# Define the path for logging the model\n",
        "model_path = \"models/sentiment_analysis\"\n",
        "\n",
        "# Log the best model to MLflow specifying the path\n",
        "mlflow.tensorflow.log_model(best_model, artifact_path=model_path)\n",
        "\n",
        "# Construct the model URI using the current MLflow run ID and the model path\n",
        "run_id = mlflow.active_run().info.run_id\n",
        "model_uri = f\"runs:/{run_id}/{model_path}\"\n",
        "\n",
        "# Register the logged model in the MLflow Model Registry\n",
        "model_details = mlflow.register_model(model_uri=model_uri, name=\"SentimentAnalysisModel\")\n",
        "\n",
        "# Transition the registered model to the 'Production' stage using the MLflow client\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "client.transition_model_version_stage(\n",
        "    name=\"SentimentAnalysisModel\",\n",
        "    version=model_details.version,\n",
        "    stage=\"Production\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Jkf-7sgMCwAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}